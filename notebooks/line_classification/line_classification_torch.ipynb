{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d852388d-57dd-4e2c-96ea-e388b733768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../src/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ce8e83f-e2dc-449e-9335-bc582d19b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5c11e6-4466-40f2-8669-bf32b1a4e961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign_no</th>\n",
       "      <th>arc_no</th>\n",
       "      <th>episode_no</th>\n",
       "      <th>episode_index</th>\n",
       "      <th>episode_label</th>\n",
       "      <th>section_no</th>\n",
       "      <th>line_no</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "      <th>nwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>223.0</td>\n",
       "      <td>2-5-18</td>\n",
       "      <td>2</td>\n",
       "      <td>1054</td>\n",
       "      <td>LIAM</td>\n",
       "      <td>Fjord, do you want to carry these, or do you w...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>274.0</td>\n",
       "      <td>3-1-19</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>LAURA</td>\n",
       "      <td>Oh, it's my glasses. Hold on. (laughter)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>295.0</td>\n",
       "      <td>3-2-17</td>\n",
       "      <td>4</td>\n",
       "      <td>710</td>\n",
       "      <td>MATT</td>\n",
       "      <td>I'll allow it for the time being, yeah.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>303.0</td>\n",
       "      <td>3-2-25</td>\n",
       "      <td>4</td>\n",
       "      <td>468</td>\n",
       "      <td>MARISHA</td>\n",
       "      <td>You're more just visiting another landscape.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>241.0</td>\n",
       "      <td>2-6-15</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>TRAVIS</td>\n",
       "      <td>As a point of clarification--</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371415</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>3-3-19</td>\n",
       "      <td>2</td>\n",
       "      <td>1258</td>\n",
       "      <td>MATT</td>\n",
       "      <td>All right, finishing FCG's go.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371416</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>266.0</td>\n",
       "      <td>3-1-11</td>\n",
       "      <td>2</td>\n",
       "      <td>415</td>\n",
       "      <td>MATT</td>\n",
       "      <td>22 points of lightning damage.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371417</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>254.0</td>\n",
       "      <td>2-6-28</td>\n",
       "      <td>4</td>\n",
       "      <td>395</td>\n",
       "      <td>LIAM</td>\n",
       "      <td>But I'm still pretty low.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371418</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>326.0</td>\n",
       "      <td>3-3-20</td>\n",
       "      <td>2</td>\n",
       "      <td>1058</td>\n",
       "      <td>MATT</td>\n",
       "      <td>All right, who's keeping watch?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371419</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1-3-37</td>\n",
       "      <td>2</td>\n",
       "      <td>675</td>\n",
       "      <td>SAM</td>\n",
       "      <td>I would like a seat at the table, if that's al...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371420 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        campaign_no  arc_no  episode_no  episode_index episode_label  \\\n",
       "0                 2       5          18          223.0        2-5-18   \n",
       "1                 3       1          19          274.0        3-1-19   \n",
       "2                 3       2          17          295.0        3-2-17   \n",
       "3                 3       2          25          303.0        3-2-25   \n",
       "4                 2       6          15          241.0        2-6-15   \n",
       "...             ...     ...         ...            ...           ...   \n",
       "371415            3       3          19          325.0        3-3-19   \n",
       "371416            3       1          11          266.0        3-1-11   \n",
       "371417            2       6          28          254.0        2-6-28   \n",
       "371418            3       3          20          326.0        3-3-20   \n",
       "371419            1       3          37           74.0        1-3-37   \n",
       "\n",
       "        section_no  line_no  speaker  \\\n",
       "0                2     1054     LIAM   \n",
       "1                2     1943    LAURA   \n",
       "2                4      710     MATT   \n",
       "3                4      468  MARISHA   \n",
       "4                4       13   TRAVIS   \n",
       "...            ...      ...      ...   \n",
       "371415           2     1258     MATT   \n",
       "371416           2      415     MATT   \n",
       "371417           4      395     LIAM   \n",
       "371418           2     1058     MATT   \n",
       "371419           2      675      SAM   \n",
       "\n",
       "                                                     line  nwords  \n",
       "0       Fjord, do you want to carry these, or do you w...      16  \n",
       "1                Oh, it's my glasses. Hold on. (laughter)       7  \n",
       "2                 I'll allow it for the time being, yeah.       8  \n",
       "3            You're more just visiting another landscape.       6  \n",
       "4                           As a point of clarification--       5  \n",
       "...                                                   ...     ...  \n",
       "371415                     All right, finishing FCG's go.       5  \n",
       "371416                     22 points of lightning damage.       5  \n",
       "371417                          But I'm still pretty low.       5  \n",
       "371418                    All right, who's keeping watch?       5  \n",
       "371419  I would like a seat at the table, if that's al...      12  \n",
       "\n",
       "[371420 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_train      = pd.read_csv('../../data/prepared_data/transcript_train.csv')\n",
    "transcript_test       = pd.read_csv('../../data/prepared_data/transcript_test.csv')\n",
    "transcript_validation = pd.read_csv('../../data/prepared_data/transcript_validation.csv')\n",
    "transcript_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a34b0333-c1c6-4e1f-be8e-b6e143ef94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_train['speakerno']      = [cast[x]['speakerno'] for x in transcript_train['speaker']]\n",
    "transcript_test['speakerno']       = [cast[x]['speakerno'] for x in transcript_test['speaker']]\n",
    "transcript_validation['speakerno'] = [cast[x]['speakerno'] for x in transcript_validation['speaker']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a2ba1a-afa6-4e00-a97a-f7a95498e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for text in data['line']:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(transcript_train), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "588d0570-3aae-4305-9a3b-cfa46f17aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 26, 5, 86, 7, 26, 18, 0, 2, 0, 14]\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6, 3, 64, 1265, 8, 37, 4, 105, 213, 2, 36, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))[:100]\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "print(text_pipeline('how do you want to do this williamchadyoung, habcldiekso?'))\n",
    "print(text_pipeline('<unk>'))\n",
    "[vocab[token] for token in tokenizer(\"I'll allow it for the time being, yeah.\")]\n",
    "# [text_pipeline(x) for x in ['how want kill', 'do to']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0f0f107b-7df3-40f9-b4fd-fd68ee796b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataset(Dataset):\n",
    "    def __init__(self, dataset, vocab, line_length):\n",
    "        self.line_length = line_length\n",
    "        self.data   = [self.tokenize_line(x) for x in dataset['line']]\n",
    "        self.labels = [cast[x]['speakerno'] for x in dataset['speaker']]\n",
    "        self.vocab  = vocab\n",
    "\n",
    "    def tokenize_line(self, line):\n",
    "        tokens = tokenizer(line)[:self.line_length]\n",
    "        return np.pad(tokens, (self.line_length - len(tokens), 0), mode='constant', constant_values='<unk>').tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.vocab(self.data[idx]), self.labels[idx]\n",
    "        # example = self.data[idx]\n",
    "        # Convert the example into numerical representations using the vocabulary\n",
    "        # numerical_tokens = [self.vocab[token] for token in example]\n",
    "        # return numerical_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e7fbab5-7e9a-4363-b9b5-b6b78ca41d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 6, 3, 64, 1265, 8, 37, 4, 105, 213, 2, 36, 1], 0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = LinesDataset(transcript_train[:10], vocab, 15)\n",
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "23936a57-c992-437a-bebc-a8cd22272491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([286,   0,   0,   0,   0,   0,   0,   0,   5,   0]),\n",
       "  tensor([  2,  56,   0,   0,   0,  26,   0,   0, 100,   0]),\n",
       "  tensor([ 26,   2,   0,   0,   0,   5,   0,   0, 140,   0]),\n",
       "  tensor([    5,     8,     6,     0,     0,    86,     0,     0, 35485,     0]),\n",
       "  tensor([86,  3,  3,  0,  0,  7,  0,  0, 89,  0]),\n",
       "  tensor([    7,    11,    64,     0,     0,    49,     0,     0,    10, 38864]),\n",
       "  tensor([  923,    61,  1265,     5,     0,    35,     0,     0, 22559,    18]),\n",
       "  tensor([ 128, 2174,    8,    3,    0,   51,    0,   62,  452,    1]),\n",
       "  tensor([ 2,  1, 37, 24,  0,  3,  0, 16,  2, 15]),\n",
       "  tensor([ 62, 244,   4, 111,   0,  24,   0,  13,   4,  20]),\n",
       "  tensor([  26,   23,  105,   22,   28,  934,    0,  284, 1825,   49]),\n",
       "  tensor([   5,    1,  213, 7466,   10,   17,   50,   12, 1819,   97]),\n",
       "  tensor([ 86,  32,   2, 201, 170,   4,   2,  38,  13, 219]),\n",
       "  tensor([  68,   95,   36, 1830,   12, 6363,    9, 1464, 1181,   89]),\n",
       "  tensor([    7,    31,     1,     1, 23724,    14,  1803,    14,    52,    14])],\n",
       " tensor([6, 1, 0, 3, 4, 5, 3, 4, 0, 1])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = LinesDataset(transcript_train[:10], vocab, 15)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e63b965-dff4-422c-ae9c-edf919b03aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        last_hidden = output[:, -1, :]\n",
    "        logits = self.fc(last_hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "41564e42-e283-4748-9d5a-28ef1e8dba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([   26,   286, 38864,     0,    28]), tensor([ 5,  2, 18, 50, 10]), tensor([ 86,  26,   1,   2, 170]), tensor([ 7,  5, 15,  9, 12]), tensor([   49,    86,    20,  1803, 23724])]\n",
      "Classes\n",
      "tensor([5, 6, 1, 3, 4])\n",
      "[tensor([ 5,  5,  6, 56, 62]), tensor([100,   3,   3,   2,  16]), tensor([140,  24,  64,   8,  13]), tensor([35485,   111,  1265,     3,   284]), tensor([89, 22,  8, 11, 12])]\n",
      "Classes\n",
      "tensor([0, 3, 0, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_classes = 8\n",
    "batch_size = 5\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the model\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create data loaders for the training and validation sets\n",
    "train_dataset = LinesDataset(transcript_train[:10], vocab, 5)\n",
    "valid_dataset = LinesDataset(transcript_validation[:10], vocab, 5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate over the training data for the specified number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    for inputs in train_loader:\n",
    "        print(inputs[0])\n",
    "        print('Classes')\n",
    "        print(inputs[1])\n",
    "        # optimizer.zero_grad()\n",
    "        # # inputs = torch.LongTensor(inputs[0])\n",
    "        # targets = inputs[1]\n",
    "        # outputs = model(inputs[0])\n",
    "        # loss = criterion(outputs.view(-1, num_classes), targets.view(-1))\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "    #     total_loss += loss.item() * len(inputs)\n",
    "    #     total_samples += len(inputs)\n",
    "\n",
    "    # # Evaluate on the validation set after every epoch\n",
    "    # model.eval()\n",
    "    # total_val_loss = 0.0\n",
    "    # total_val_samples = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for inputs in valid_loader:\n",
    "    #         inputs = torch.LongTensor(inputs)\n",
    "    #         targets = inputs.clone()\n",
    "    #         outputs = model(inputs)\n",
    "    #         val_loss = criterion(outputs.view(-1, num_classes), targets.view(-1))\n",
    "\n",
    "    #         total_val_loss += val_loss.item() * len(inputs)\n",
    "    #         total_val_samples += len(inputs)\n",
    "\n",
    "    # avg_loss = total_loss / total_samples\n",
    "    # avg_val_loss = total_val_loss / total_val_samples\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3494afc0-0b92-4b03-b440-952adca82f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "def yield_data(data):\n",
    "    for _, row in data.iterrows():\n",
    "        yield(row['speakerno'], row['line'])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    yield_data(transcript_train),\n",
    "    batch_size = 8,\n",
    "    shuffle    = False,\n",
    "    collate_fn = collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367f7322-4b43-48db-83da-3bf7d49c33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfeb491c-2458-4bfe-9b4d-dedd69c884d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "48971\n"
     ]
    }
   ],
   "source": [
    "train_iter = yield_data(transcript_train)\n",
    "num_class  = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize     = 64\n",
    "model      = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "print(num_class)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944a0a4e-87bb-4d7e-91aa-dd03c294b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8495e8a3-45a4-47a4-a437-c22b60ec9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS     = 10  # epoch\n",
    "LR         = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion  = torch.nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_iter = yield_data(transcript_train[:100])\n",
    "valid_iter = yield_data(transcript_validation[:100])\n",
    "test_iter  = yield_data(transcript_test[:100])\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "valid_dataset = to_map_style_dataset(valid_iter)\n",
    "test_dataset  = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle    = True,\n",
    "    collate_fn = collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle    = True,\n",
    "    collate_fn = collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle    = True,\n",
    "    collate_fn = collate_batch\n",
    ")\n",
    "\n",
    "#train(train_dataloader)\n",
    "\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train(train_dataloader)\n",
    "#     accu_val = evaluate(valid_dataloader)\n",
    "#     if total_accu is not None and total_accu > accu_val:\n",
    "#         scheduler.step()\n",
    "#     else:\n",
    "#         total_accu = accu_val\n",
    "#     print(\"-\" * 59)\n",
    "#     print(\n",
    "#         \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "#         \"valid accuracy {:8.3f} \".format(\n",
    "#             epoch, time.time() - epoch_start_time, accu_val\n",
    "#         )\n",
    "#     )\n",
    "#     print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6712ce9-b170-4c0a-9a7f-27dd5338543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72559903-2c87-4b70-85bd-f55875e37fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
