{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d852388d-57dd-4e2c-96ea-e388b733768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../src/config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce8e83f-e2dc-449e-9335-bc582d19b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5c11e6-4466-40f2-8669-bf32b1a4e961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign_no</th>\n",
       "      <th>arc_no</th>\n",
       "      <th>episode_no</th>\n",
       "      <th>episode_index</th>\n",
       "      <th>episode_label</th>\n",
       "      <th>section_no</th>\n",
       "      <th>line_no</th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "      <th>nwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>223.0</td>\n",
       "      <td>2-5-18</td>\n",
       "      <td>2</td>\n",
       "      <td>1054</td>\n",
       "      <td>LIAM</td>\n",
       "      <td>Fjord, do you want to carry these, or do you w...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>274.0</td>\n",
       "      <td>3-1-19</td>\n",
       "      <td>2</td>\n",
       "      <td>1943</td>\n",
       "      <td>LAURA</td>\n",
       "      <td>Oh, it's my glasses. Hold on. (laughter)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>295.0</td>\n",
       "      <td>3-2-17</td>\n",
       "      <td>4</td>\n",
       "      <td>710</td>\n",
       "      <td>MATT</td>\n",
       "      <td>I'll allow it for the time being, yeah.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>303.0</td>\n",
       "      <td>3-2-25</td>\n",
       "      <td>4</td>\n",
       "      <td>468</td>\n",
       "      <td>MARISHA</td>\n",
       "      <td>You're more just visiting another landscape.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>241.0</td>\n",
       "      <td>2-6-15</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>TRAVIS</td>\n",
       "      <td>As a point of clarification--</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371415</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>325.0</td>\n",
       "      <td>3-3-19</td>\n",
       "      <td>2</td>\n",
       "      <td>1258</td>\n",
       "      <td>MATT</td>\n",
       "      <td>All right, finishing FCG's go.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371416</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>266.0</td>\n",
       "      <td>3-1-11</td>\n",
       "      <td>2</td>\n",
       "      <td>415</td>\n",
       "      <td>MATT</td>\n",
       "      <td>22 points of lightning damage.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371417</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>254.0</td>\n",
       "      <td>2-6-28</td>\n",
       "      <td>4</td>\n",
       "      <td>395</td>\n",
       "      <td>LIAM</td>\n",
       "      <td>But I'm still pretty low.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371418</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>326.0</td>\n",
       "      <td>3-3-20</td>\n",
       "      <td>2</td>\n",
       "      <td>1058</td>\n",
       "      <td>MATT</td>\n",
       "      <td>All right, who's keeping watch?</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371419</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1-3-37</td>\n",
       "      <td>2</td>\n",
       "      <td>675</td>\n",
       "      <td>SAM</td>\n",
       "      <td>I would like a seat at the table, if that's al...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>371420 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        campaign_no  arc_no  episode_no  episode_index episode_label  \\\n",
       "0                 2       5          18          223.0        2-5-18   \n",
       "1                 3       1          19          274.0        3-1-19   \n",
       "2                 3       2          17          295.0        3-2-17   \n",
       "3                 3       2          25          303.0        3-2-25   \n",
       "4                 2       6          15          241.0        2-6-15   \n",
       "...             ...     ...         ...            ...           ...   \n",
       "371415            3       3          19          325.0        3-3-19   \n",
       "371416            3       1          11          266.0        3-1-11   \n",
       "371417            2       6          28          254.0        2-6-28   \n",
       "371418            3       3          20          326.0        3-3-20   \n",
       "371419            1       3          37           74.0        1-3-37   \n",
       "\n",
       "        section_no  line_no  speaker  \\\n",
       "0                2     1054     LIAM   \n",
       "1                2     1943    LAURA   \n",
       "2                4      710     MATT   \n",
       "3                4      468  MARISHA   \n",
       "4                4       13   TRAVIS   \n",
       "...            ...      ...      ...   \n",
       "371415           2     1258     MATT   \n",
       "371416           2      415     MATT   \n",
       "371417           4      395     LIAM   \n",
       "371418           2     1058     MATT   \n",
       "371419           2      675      SAM   \n",
       "\n",
       "                                                     line  nwords  \n",
       "0       Fjord, do you want to carry these, or do you w...      16  \n",
       "1                Oh, it's my glasses. Hold on. (laughter)       7  \n",
       "2                 I'll allow it for the time being, yeah.       8  \n",
       "3            You're more just visiting another landscape.       6  \n",
       "4                           As a point of clarification--       5  \n",
       "...                                                   ...     ...  \n",
       "371415                     All right, finishing FCG's go.       5  \n",
       "371416                     22 points of lightning damage.       5  \n",
       "371417                          But I'm still pretty low.       5  \n",
       "371418                    All right, who's keeping watch?       5  \n",
       "371419  I would like a seat at the table, if that's al...      12  \n",
       "\n",
       "[371420 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_train      = pd.read_csv('../../data/prepared_data/transcript_train.csv')\n",
    "transcript_test       = pd.read_csv('../../data/prepared_data/transcript_test.csv')\n",
    "transcript_validation = pd.read_csv('../../data/prepared_data/transcript_validation.csv')\n",
    "transcript_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a34b0333-c1c6-4e1f-be8e-b6e143ef94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_train['speakerno']      = [cast[x]['speakerno'] for x in transcript_train['speaker']]\n",
    "transcript_test['speakerno']       = [cast[x]['speakerno'] for x in transcript_test['speaker']]\n",
    "transcript_validation['speakerno'] = [cast[x]['speakerno'] for x in transcript_validation['speaker']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a2ba1a-afa6-4e00-a97a-f7a95498e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for text in data['line']:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(transcript_train), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "588d0570-3aae-4305-9a3b-cfa46f17aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 26, 5, 86, 7, 26, 18, 0, 2, 0, 14]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "print(text_pipeline('how do you want to do this williamchadyoung, habcldiekso?'))\n",
    "print(text_pipeline('<unk>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3494afc0-0b92-4b03-b440-952adca82f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "def yield_data(data):\n",
    "    for _, row in data.iterrows():\n",
    "        yield(row['speakerno'], row['line'])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    yield_data(transcript_train),\n",
    "    batch_size = 8,\n",
    "    shuffle    = False,\n",
    "    collate_fn = collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "367f7322-4b43-48db-83da-3bf7d49c33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb491c-2458-4bfe-9b4d-dedd69c884d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = yield_data(transcript_train)\n",
    "num_class  = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize     = 64\n",
    "model      = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "print(num_class)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a0a4e-87bb-4d7e-91aa-dd03c294b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495e8a3-45a4-47a4-a437-c22b60ec9ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS     = 10  # epoch\n",
    "LR         = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion  = torch.nn.CrossEntropyLoss()\n",
    "optimizer  = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler  = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "train_iter = yield_data(transcript_train[:1000])\n",
    "valid_iter = yield_data(transcript_validation[:1000])\n",
    "test_iter  = yield_data(transcript_test[:1000])\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "valid_dataset = to_map_style_dataset(valid_iter)\n",
    "test_dataset  = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle    = True,\n",
    "    collate_fn = collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle    = True,\n",
    "    collate_fn = collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle    = True,\n",
    "    collate_fn = collate_batch\n",
    ")\n",
    "\n",
    "train(train_dataloader)\n",
    "\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     epoch_start_time = time.time()\n",
    "#     train(train_dataloader)\n",
    "#     accu_val = evaluate(valid_dataloader)\n",
    "#     if total_accu is not None and total_accu > accu_val:\n",
    "#         scheduler.step()\n",
    "#     else:\n",
    "#         total_accu = accu_val\n",
    "#     print(\"-\" * 59)\n",
    "#     print(\n",
    "#         \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "#         \"valid accuracy {:8.3f} \".format(\n",
    "#             epoch, time.time() - epoch_start_time, accu_val\n",
    "#         )\n",
    "#     )\n",
    "#     print(\"-\" * 59)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
